{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTS\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FUNCIONS\n",
    "\"\"\"\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset = pd.read_csv(path, header=0, delimiter=',')\n",
    "    return dataset\n",
    "\n",
    "def clean_dataset(dataset: pd.core.frame.DataFrame, type = 'default') -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Funció que processa els valors \"NULLS\" d'un dataset aplicant l'estratègia o tractament especificat\n",
    "    :param dataset: DataFrame amb la informació que es vol filtrar\n",
    "    :param type: Indica el tipus de tractament dels  \"NaN\"\n",
    "    :return: DataFrame aplicant el mètode especificat a totes les línies amb valors \"NaN\"\n",
    "    \"\"\"\n",
    "    if type == 'default':\n",
    "        # Deletes all rows with missing values\n",
    "        return dataset.dropna()\n",
    "    elif type == 'backfill':\n",
    "        # Applies pandas method of backfilling\n",
    "        return dataset.fillna(method='backfill', axis=1)\n",
    "    elif type == 'mean':\n",
    "        # Replaces missing values with the mean of the column\n",
    "        return dataset.fillna(dataset.mean())\n",
    "    else:\n",
    "        print(\"::-> ERROR : clean_dataset - \" + str(type) + \" is not a valid option...\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dataset = load_dataset('pokemon.csv')\n",
    "\n",
    "# Feature Selection : Elimianció atributs que no aporten informació\n",
    "dataset = dataset.drop(['japanese_name', 'name', 'pokedex_number'], axis=1)\n",
    "\n",
    "# Eliminació de classification, doncs és un atribut que expressa si és legendary o no i aquest és el nostre objectiu a determinar\n",
    "dataset = dataset.drop(['classfication'], axis=1)\n",
    "\n",
    "# Eliminació atribut que complica el nostre model\n",
    "# Null en aquesta linia convertir-ho en un tipus que no te\n",
    "# dataset = dataset.drop(['type2'], axis=1)\n",
    "\n",
    "# Codificació dels atributs categòrics amb mètode OneHotEncoder:\n",
    "\n",
    "# Categoritzar Type1\n",
    "# Definim encoder\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "# Creem variable auxiliar per guardar la informació del atribut type1\n",
    "new_data_1 = pd.DataFrame (encoder.fit_transform(dataset[['type1']]))\n",
    "# Assignem els noms a les columnes\n",
    "new_data_1.columns = encoder.get_feature_names(['type1'])\n",
    "# Eliminem la columna del dataset\n",
    "dataset.drop(['type1'] ,axis=1, inplace=True)\n",
    "\n",
    "# Categoritzar Type2\n",
    "# Modifiquem nom de la columna type 2 a type 1 per tal de poder fer un merge posteriorment\n",
    "# Utilitzarem el mateix procés que amb la variable type1\n",
    "dataset = dataset.rename(columns = {'type2' : 'type1'})\n",
    "new_data_2 = pd.DataFrame (encoder.fit_transform(dataset[['type1']]))\n",
    "new_data_2.columns = encoder.get_feature_names(['type1'])\n",
    "dataset.drop(['type1'] ,axis=1, inplace=True)\n",
    "\n",
    "# Sumem les dues taules type\n",
    "new_data_type = new_data_1 + new_data_2\n",
    "# Afegim al final del dataset les taules type1 i type2 codificades amb OneHotEncoder\n",
    "dataset = pd.concat([dataset, new_data_type], axis=1)\n",
    "\n",
    "# Eliminem la columna type amb valors NULLS:\n",
    "dataset = dataset.drop(['type1_nan'], axis=1)\n",
    "\n",
    "# Error en linia 773. Parametres no valids\n",
    "# dataset = dataset.drop(index=773, axis=0)\n",
    "dataset['']\n",
    "\n",
    "# Tractament valors NULLS -> Aplicarem mètode de la mitjana\n",
    "dataset = clean_dataset(dataset, type='mean')\n",
    "\n",
    "\n",
    "\n",
    "import ast\n",
    "ab = dataset['abilities'].to_numpy()\n",
    "a = []\n",
    "for row in ab:\n",
    "    row = ast.literal_eval(row)\n",
    "    a.append(row)\n",
    "\n",
    "a = np.array(a).flatten()\n",
    "\n",
    "flat_list = [item for sublist in a for item in sublist]\n",
    "\n",
    "abilitats_uniques = np.unique(flat_list).reshape(-1,1)\n",
    "\n",
    "# Declarem encoder tipus OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# Declarem Dataframe auxiliar amb les noves columnes\n",
    "enc_df = pd.DataFrame(enc.fit_transform(abilitats_uniques).toarray(), columns=['abilities_' + str(i[0]) for i in abilitats_uniques])\n",
    "# Ajuntem al dataframe original amb el dataframe auxiliar\n",
    "dataset = dataset.join(enc_df)\n",
    "dataset = dataset.drop(['abilities'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = dataset.values[:,34]\n",
    "X = dataset.drop(['is_legendary'] ,axis=1, inplace=True)\n",
    "X = dataset.values[:,:]\n",
    "\n",
    "x_t, x_v, y_t, y_v = train_test_split(X, y, train_size=0.3)\n",
    "\n",
    "param_grid = [\n",
    "    {\"C\": np.logspace(-3,3,7), \"penalty\":[\"l1\"], 'solver': ['saga'], 'multi_class': ['auto', 'ovr', 'multinomial']},\n",
    "    {\"C\": np.logspace(-3,3,7), \"penalty\":[\"l2\", \"none\"], 'solver': ['newton-cg','sag', 'lbfgs'], 'multi_class': ['auto', 'ovr', 'multinomial']}\n",
    "\n",
    "]\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}